{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Fitting Energy-Based Models from Experimental Data\n\nGiven experimental measurements of system equilibria, this notebook shows how to fit THRML Ising model parameters to reproduce observed behavior.\n\n**Method:** solve analytically using the mean-field approximation $\\theta = \\sigma(2b)$, then validate against THRML samples.\n\nRelates to [#29](https://github.com/extropic-ai/thrml/issues/29)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import jax\nimport jax.numpy as jnp\nimport jax.random\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import expit  # sigmoid\n\nfrom thrml.block_management import Block\nfrom thrml.block_sampling import sample_states, SamplingSchedule\nfrom thrml.models.ising import IsingEBM, IsingSamplingProgram\nfrom thrml.pgm import SpinNode"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**The fitting problem**\n\nWe have a system with $K$ binary spins. The mean spin fraction $\\theta = \\frac{1}{K}\\sum_i \\mathbb{1}[s_i = +1]$ is the observable. Two competing forces shape $\\theta$:\n\n- **Drive** (bias $b_\\alpha > 0$): pushes spins toward $+1$\n- **Constraint** (bias $b_\\gamma > 0$, scaled by $c \\in [0,1]$): pushes spins toward $-1$\n\nNet per-spin bias: $b = b_\\alpha - c \\cdot b_\\gamma$. Mean-field equilibrium: $\\theta^* = \\sigma(2b) = 1/(1 + e^{-2b})$.\n\n**Experimental targets:**\n\n| Condition | Constraint $c$ | Target $\\theta$ |\n|-----------|:--------------:|:---------------:|\n| A (no constraint) | 0.0 | 0.80 |\n| B (partial) | 0.5 | 0.26 |\n| C (full constraint) | 1.0 | 0.00 |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 16  # spins per agent\n",
    "seed = 42\n",
    "\n",
    "# Experimental targets\n",
    "conditions = [\n",
    "    {\"name\": \"A (no constraint)\", \"c\": 0.0, \"target_theta\": 0.80},\n",
    "    {\"name\": \"B (partial)\", \"c\": 0.5, \"target_theta\": 0.26},\n",
    "    {\"name\": \"C (full constraint)\", \"c\": 1.0, \"target_theta\": 0.00},\n",
    "]\n",
    "\n",
    "# Held-out validation targets (not used during fitting)\n",
    "validation = [\n",
    "    {\"name\": \"V1 (c=0.25)\", \"c\": 0.25, \"target_theta\": 0.55},\n",
    "    {\"name\": \"V2 (c=0.75)\", \"c\": 0.75, \"target_theta\": 0.10},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Parameterized model**\n\nIsing chain where all spins share the same net bias $b = b_\\alpha - c \\cdot b_\\gamma$, with weak ferromagnetic coupling for coherence."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(b_alpha, b_gamma, c, K=16):\n",
    "    \"\"\"Build an Ising EBM with drive bias b_alpha and constraint bias b_gamma.\n",
    "\n",
    "    Net per-spin bias: b = b_alpha - c * b_gamma\n",
    "    Equilibrium: theta = sigma(2*b) = 1/(1+exp(-2*b))\n",
    "    \"\"\"\n",
    "    nodes = [SpinNode() for _ in range(K)]\n",
    "    edges = [(nodes[i], nodes[i + 1]) for i in range(K - 1)]\n",
    "\n",
    "    net_bias = b_alpha - c * b_gamma\n",
    "    biases = jnp.full(K, net_bias)\n",
    "\n",
    "    # Weak ferromagnetic coupling for spin coherence\n",
    "    J_within = 0.02 / K\n",
    "    edge_weights = jnp.full(K - 1, J_within)\n",
    "\n",
    "    beta = jnp.array(1.0)\n",
    "    model = IsingEBM(nodes, edges, biases, edge_weights, beta)\n",
    "    return model, nodes\n",
    "\n",
    "\n",
    "def sample_theta(b_alpha, b_gamma, c, K=16, n_samples=200, n_warmup=500, rng_key=None):\n",
    "    \"\"\"Sample from the model and return mean theta.\"\"\"\n",
    "    model, nodes = build_model(b_alpha, b_gamma, c, K)\n",
    "\n",
    "    # Chromatic blocking (2 colors for 1D chain)\n",
    "    even = [nodes[i] for i in range(0, K, 2)]\n",
    "    odd = [nodes[i] for i in range(1, K, 2)]\n",
    "    free_blocks = [Block(even), Block(odd)]\n",
    "\n",
    "    program = IsingSamplingProgram(model, free_blocks, [])\n",
    "    schedule = SamplingSchedule(n_warmup, n_samples, 5)\n",
    "\n",
    "    init_state = [\n",
    "        jnp.zeros(len(even), dtype=jnp.bool_),\n",
    "        jnp.zeros(len(odd), dtype=jnp.bool_),\n",
    "    ]\n",
    "\n",
    "    if rng_key is None:\n",
    "        rng_key = jax.random.key(0)\n",
    "\n",
    "    samples = sample_states(\n",
    "        rng_key, program, schedule, init_state, [], [Block(nodes)]\n",
    "    )\n",
    "    spins = samples[0]  # shape (n_samples, K), boolean\n",
    "    theta = jnp.mean(spins.astype(jnp.float32), axis=-1)  # per-sample theta\n",
    "    return float(jnp.mean(theta)), float(jnp.std(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Analytic mean-field solution**\n\nFor large $K$, the mean-field approximation $\\theta^* = \\sigma(2b)$ is tight. We fit $b_\\alpha$ and $b_\\gamma$ by minimizing the squared error on the analytic prediction \u2014 no THRML sampling required, just three function evaluations per iteration."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def analytic_theta(b_alpha, b_gamma, c):\n    \"\"\"Mean-field prediction: theta = sigma(2*(b_alpha - c*b_gamma)).\"\"\"\n    return float(expit(2.0 * (b_alpha - c * b_gamma)))\n\n\ndef analytic_loss(params, conditions):\n    b_alpha, b_gamma = params\n    eps = 1e-3  # floor for target=0.00\n    total = 0.0\n    thetas = []\n    for cond in conditions:\n        target = max(cond[\"target_theta\"], eps)\n        theta = analytic_theta(b_alpha, b_gamma, cond[\"c\"])\n        total += (theta - target) ** 2\n        thetas.append(theta)\n    # rank-order penalty\n    for i in range(len(thetas) - 1):\n        if thetas[i] <= thetas[i + 1]:\n            total += 1.0\n    return total\n\n\nresult = minimize(analytic_loss, [0.5, 2.0], args=(conditions,), method=\"Nelder-Mead\",\n                  options={\"xatol\": 1e-6, \"fatol\": 1e-8, \"maxiter\": 10_000})\nb_alpha_fit, b_gamma_fit = result.x\n\nprint(f\"Fitted: b_alpha={b_alpha_fit:.4f}, b_gamma={b_gamma_fit:.4f}\")\nprint(f\"Analytic loss: {result.fun:.6f}\")\nprint()\nprint(f\"{'Condition':<20} {'Target':>8} {'Analytic':>9}\")\nprint(\"-\" * 40)\nfor cond in conditions:\n    theta_mf = analytic_theta(b_alpha_fit, b_gamma_fit, cond[\"c\"])\n    print(f\"{cond['name']:<20} {cond['target_theta']:>8.3f} {theta_mf:>9.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**THRML validation**\n\nWith fitted parameters in hand, we run one THRML sampling pass per condition to confirm the sampler matches the mean-field prediction. This is a single round of sampling \u2014 not an optimization loop."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "key = jax.random.key(seed)\n\nprint(f\"{'Condition':<20} {'Target':>8} {'Analytic':>9} {'THRML':>8} {'\u0394 (MF)':>8}\")\nprint(\"-\" * 56)\n\nthrml_thetas = []\nfor cond in conditions:\n    key, subkey = jax.random.split(key)\n    theta_mf = analytic_theta(b_alpha_fit, b_gamma_fit, cond[\"c\"])\n    theta_thrml, _ = sample_theta(b_alpha_fit, b_gamma_fit, cond[\"c\"], rng_key=subkey)\n    thrml_thetas.append(theta_thrml)\n    delta_mf = abs(theta_thrml - theta_mf)\n    print(f\"{cond['name']:<20} {cond['target_theta']:>8.3f} {theta_mf:>9.3f} {theta_thrml:>8.3f} {delta_mf:>8.3f}\")\n\nrank_ok = all(thrml_thetas[i] > thrml_thetas[i + 1] for i in range(len(thrml_thetas) - 1))\nprint(f\"\\nRank order preserved: {rank_ok}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Held-out validation:\")\nprint(f\"{'Condition':<20} {'Target':>8} {'Analytic':>9} {'THRML':>8} {'Error':>8}\")\nprint(\"-\" * 56)\n\nfor cond in validation:\n    key, subkey = jax.random.split(key)\n    theta_mf = analytic_theta(b_alpha_fit, b_gamma_fit, cond[\"c\"])\n    theta_thrml, _ = sample_theta(b_alpha_fit, b_gamma_fit, cond[\"c\"], rng_key=subkey)\n    err = abs(theta_thrml - cond[\"target_theta\"])\n    print(f\"{cond['name']:<20} {cond['target_theta']:>8.3f} {theta_mf:>9.3f} {theta_thrml:>8.3f} {err:>8.3f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Theta vs constraint strength: THRML samples, mean-field, and targets\nc_dense = np.linspace(0, 1, 20)\ntheta_thrml_curve = []\n\nfor c_val in c_dense:\n    key, subkey = jax.random.split(key)\n    theta, _ = sample_theta(b_alpha_fit, b_gamma_fit, c_val, rng_key=subkey)\n    theta_thrml_curve.append(theta)\n\ntheta_mf_curve = [analytic_theta(b_alpha_fit, b_gamma_fit, c) for c in c_dense]\n\nfig, ax = plt.subplots(figsize=(8, 5))\nax.plot(c_dense, theta_thrml_curve, \"b-\", linewidth=2, label=\"THRML samples\")\nax.plot(c_dense, theta_mf_curve, \"k--\", alpha=0.5, label=\"Analytic (mean-field)\")\n\n# Training targets\nax.scatter([cond[\"c\"] for cond in conditions],\n           [cond[\"target_theta\"] for cond in conditions],\n           c=\"red\", s=80, zorder=5, label=\"Training targets\")\n# Validation targets\nax.scatter([cond[\"c\"] for cond in validation],\n           [cond[\"target_theta\"] for cond in validation],\n           c=\"green\", s=80, marker=\"^\", zorder=5, label=\"Validation targets\")\n\nax.set_xlabel(\"Constraint strength c\")\nax.set_ylabel(r\"Equilibrium $\\theta$\")\nax.set_title(f\"Fitted model  (b_alpha={b_alpha_fit:.3f}, b_gamma={b_gamma_fit:.3f})\")\nax.legend()\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Discussion**\n\nGeneral pattern for fitting THRML Ising parameters from experimental data:\n\n1. **Mean-field pre-fit** \u2014 minimize $\\sum_i(\\sigma(2b_i) - \\theta_i^*)^2$ analytically. Fast (no sampling), tight for large $K$.\n2. **THRML validation** \u2014 one sampling pass per condition confirms the sampler matches the mean-field prediction.\n3. **Out-of-sample test** \u2014 held-out conditions verify the parameters generalize.\n\n**When this works:** observable $\\theta$ is smooth in the parameters, $K$ is large enough that mean-field is tight, and the energy function is well-specified.\n\n**When it doesn't:** high-dimensional parameter spaces (use gradient-based methods), or when mean-field is a poor approximation (small $K$, strong coupling). For neural-network-parameterized EBMs, consider contrastive divergence via `equinox` + `optax`."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}