{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Time-Varying Potentials in THRML\n",
    "\n",
    "Energy-based models often need non-static energy landscapes: simulated annealing, scheduled biases, or dynamically injected constraints. This notebook demonstrates patterns for time-varying potentials in THRML.\n",
    "\n",
    "**Patterns covered:**\n",
    "1. Step-function schedules (discrete constraint injection)\n",
    "2. One-shot transitions (sudden potential change)\n",
    "3. Exponential decay (constraint erosion)\n",
    "4. Persistent bias (static reference case)\n",
    "5. Comparing schedule strategies: which converges fastest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-std",
   "metadata": {},
   "outputs": [],
   "source": "import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np"
  },
  {
   "cell_type": "code",
   "id": "koxus7bwbb",
   "source": "from thrml.block_management import Block\nfrom thrml.block_sampling import sample_states, SamplingSchedule\nfrom thrml.models.ising import IsingEBM, IsingSamplingProgram\nfrom thrml.pgm import SpinNode",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "z6ys0rinup",
   "source": "**Why time-varying potentials?**\n\nMany applications of EBMs require non-static energy landscapes:\n\n- **Simulated annealing:** gradually reduce temperature to find ground states\n- **Constraint injection:** introduce external fields mid-run to steer the system\n- **Scheduled biases:** ramp potentials to model changing environments\n\nTHRML's existing examples show static energy functions. This notebook demonstrates patterns for updating biases between sampling rounds, and compares strategies to answer: **does schedule shape matter, or just the final energy?**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "xghz8mwkd98",
   "source": "**Schedule construction utilities**\n\nFour schedule types: persistent (constant), one-shot (step function at midpoint), iterative steps (gradual ramp), and exponential decay.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "vf2uy3ufbe",
   "source": "def make_persistent_schedule(n_rounds: int, c: float = 1.0) -> np.ndarray:\n    \"\"\"Constant constraint from round 0.\"\"\"\n    return np.full(n_rounds, c)\n\n\ndef make_oneshot_schedule(n_rounds: int, onset_round: int = 50,\n                          c_final: float = 1.0) -> np.ndarray:\n    \"\"\"Step function: zero until onset_round, then c_final.\"\"\"\n    schedule = np.zeros(n_rounds)\n    schedule[onset_round:] = c_final\n    return schedule\n\n\ndef make_step_schedule(n_rounds: int, n_steps: int = 4,\n                       c_final: float = 1.0) -> np.ndarray:\n    \"\"\"Iterative ramp: n_steps equal increments reaching c_final at midpoint.\"\"\"\n    schedule = np.zeros(n_rounds)\n    onset = n_rounds // 2\n    step_size = c_final / n_steps\n    rounds_per_step = onset // n_steps\n\n    for s in range(n_steps):\n        start = s * rounds_per_step\n        c_val = (s + 1) * step_size\n        schedule[start:] = np.minimum(schedule[start:] + 0, c_val)\n        schedule[start:onset] = c_val\n    schedule[onset:] = c_final\n    return schedule\n\n\ndef make_decay_schedule(n_rounds: int, kappa: float = 0.05,\n                        c_init: float = 1.0) -> np.ndarray:\n    \"\"\"Exponential decay: c(t) = c_init * exp(-kappa * t).\"\"\"\n    return c_init * np.exp(-kappa * np.arange(n_rounds))\n\n\n# Visualize all four schedules\nn_rounds = 100\nschedules = {\n    \"Persistent (c=1)\": make_persistent_schedule(n_rounds),\n    \"One-shot (R50)\": make_oneshot_schedule(n_rounds, onset_round=50),\n    \"4-step ramp\": make_step_schedule(n_rounds, n_steps=4),\n    \"8-step ramp\": make_step_schedule(n_rounds, n_steps=8),\n    \"Exponential decay\": make_decay_schedule(n_rounds, kappa=0.05),\n}\n\nfig, axs = plt.subplots(figsize=(10, 4))\nfor name, sched in schedules.items():\n    axs.plot(sched, label=name, linewidth=2 if \"Persistent\" in name else 1.5)\naxs.set_xlabel(\"Round\")\naxs.set_ylabel(\"Constraint strength $c(t)$\")\naxs.set_title(\"Constraint schedule types\")\naxs.legend(fontsize=9)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8zhrbpamkbs",
   "source": "**The pattern: per-round model rebuilding**\n\nTHRML's `IsingEBM` takes static biases. To implement time-varying potentials, we rebuild the model at each round with updated biases and sample for a small number of steps, carrying forward the spin state from the previous round.\n\nThis is the general pattern for any time-varying potential in THRML: the energy function changes between sampling rounds, but within each round the sampler runs at fixed biases.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "j0pkxcvgmx",
   "source": "K = 16\nTHETA_UU = 0.85\nTHETA_GG = 0.06\n\n\ndef build_ising_at_constraint(c: float):\n    \"\"\"Build IsingEBM with a given constraint level c in [0, 1].\"\"\"\n    eps = 1e-6\n    b_drift = 0.5 * np.log(THETA_UU / (1 - THETA_UU))\n    b_gg = 0.5 * np.log(max(THETA_GG, eps) / max(1 - THETA_GG, eps))\n    b_constraint = b_drift - b_gg\n\n    nodes = [SpinNode() for _ in range(K)]\n    biases = jnp.full(K, b_drift - b_constraint * c)\n\n    # 1-D chain with nearest-neighbor coupling\n    edges = [(nodes[i], nodes[i + 1]) for i in range(K - 1)]\n    weights = jnp.full(len(edges), 0.1)\n\n    ebm = IsingEBM(\n        nodes=nodes, edges=edges, biases=biases,\n        weights=weights, beta=jnp.array(1.0),\n    )\n    return ebm, nodes\n\n\ndef simulate_schedule(constraint_schedule, steps_per_round: int = 20,\n                      seed: int = 4242):\n    \"\"\"Run THRML sampling with a time-varying constraint schedule.\n\n    At each round, rebuild the IsingEBM with updated biases and sample\n    for steps_per_round steps, carrying forward the spin state.\n\n    Returns theta trajectory: array of shape (n_rounds,).\n    \"\"\"\n    n_rounds = len(constraint_schedule)\n    key = jax.random.key(seed)\n    theta_traj = np.zeros(n_rounds)\n\n    # Initial state: all spins down\n    current_state = None\n\n    for r in range(n_rounds):\n        c = constraint_schedule[r]\n        ebm, nodes = build_ising_at_constraint(c)\n        blocks = [Block([node]) for node in nodes]\n\n        program = IsingSamplingProgram(\n            ebm=ebm, free_blocks=blocks, clamped_blocks=[],\n        )\n        schedule = SamplingSchedule(\n            n_warmup=0 if r > 0 else 50,  # warmup only on first round\n            n_samples=1,\n            steps_per_sample=steps_per_round,\n        )\n\n        key, subkey = jax.random.split(key)\n        if current_state is None:\n            init_state = [jnp.array([False]) for _ in nodes]\n        else:\n            init_state = current_state\n\n        samples = sample_states(\n            key=subkey, program=program, schedule=schedule,\n            init_state_free=init_state, state_clamp=[],\n            nodes_to_sample=blocks,\n        )\n\n        # Extract theta and save state for next round\n        spins = jnp.stack([s[0, 0] for s in samples])\n        theta_traj[r] = float(jnp.mean(spins.astype(jnp.float32)))\n        current_state = [s[0:1, :] for s in samples]\n\n    return theta_traj",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "qlgpnzk15e",
   "source": "**Comparing schedule strategies**\n\nWe run five conditions that reach the same final constraint ($c = 1$) via different paths, plus a no-constraint baseline and an exponential decay. The question: does the path to full constraint matter?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "pkwus786ya",
   "source": "n_rounds = 100\n\ntest_schedules = {\n    \"No constraint\": np.zeros(n_rounds),\n    \"Persistent (full)\": make_persistent_schedule(n_rounds),\n    \"One-shot (R50)\": make_oneshot_schedule(n_rounds, onset_round=50),\n    \"4-step ramp\": make_step_schedule(n_rounds, n_steps=4),\n    \"8-step ramp\": make_step_schedule(n_rounds, n_steps=8),\n    \"Exponential decay\": make_decay_schedule(n_rounds, kappa=0.03),\n}\n\ntrajectories = {}\nprint(\"Running schedule comparison:\\n\")\nfor name, sched in test_schedules.items():\n    theta_traj = simulate_schedule(sched, seed=4242)\n    trajectories[name] = theta_traj\n    final_theta = theta_traj[-1]\n    mean_last10 = np.mean(theta_traj[-10:])\n    var_last10 = np.var(theta_traj[-10:])\n    print(f\"  {name:22s}: final theta = {final_theta:.4f}  \"\n          f\"mean(last 10) = {mean_last10:.4f}  var(last 10) = {var_last10:.6f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "j9amhxlljlb",
   "source": "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel 1: Trajectories\ncolors = plt.cm.tab10(np.linspace(0, 1, len(trajectories)))\nfor (name, traj), color in zip(trajectories.items(), colors):\n    axes[0].plot(traj, label=name, color=color,\n                 linewidth=2 if \"Persistent\" in name else 1.2)\naxes[0].set_xlabel(\"Round\")\naxes[0].set_ylabel(r\"$\\theta$ (order parameter)\")\naxes[0].set_title(\"Trajectories under different constraint schedules\")\naxes[0].legend(fontsize=8, loc=\"upper right\")\n\n# Panel 2: Final-10-round statistics\nnames = list(trajectories.keys())\nmeans = [np.mean(trajectories[n][-10:]) for n in names]\nvariances = [np.var(trajectories[n][-10:]) for n in names]\n\nx = np.arange(len(names))\naxes[1].bar(x, means, color=[colors[i] for i in range(len(names))], alpha=0.7)\naxes[1].set_xticks(x)\naxes[1].set_xticklabels(names, rotation=35, ha=\"right\", fontsize=8)\naxes[1].set_ylabel(r\"Mean $\\theta$ (last 10 rounds)\")\naxes[1].set_title(\"Final equilibrium by schedule type\")\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "s3w9srqpas",
   "source": "**One-shot rebound effect**\n\nA striking feature of the one-shot schedule: when constraint is applied suddenly at round 50, the system drops but then **rebounds** \u2014 the order parameter partially recovers. This is experimentally confirmed (3/3 trials in the original study). It reflects the system's inertia: established correlations resist sudden potential changes.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "w5lthl2mxe",
   "source": "os_traj = trajectories[\"One-shot (R50)\"]\npers_traj = trajectories[\"Persistent (full)\"]\n\nfig, axs = plt.subplots(figsize=(10, 5))\naxs.plot(os_traj, label=\"One-shot (R50)\", linewidth=2, color=\"C1\")\naxs.plot(pers_traj, label=\"Persistent (full)\", linewidth=2, color=\"C2\",\n         linestyle=\"--\")\naxs.axvline(50, color=\"gray\", linestyle=\":\", alpha=0.5, label=\"Constraint onset\")\n\n# Mark rebound region\nif len(os_traj) > 60:\n    drop_val = os_traj[52] if len(os_traj) > 52 else os_traj[50]\n    rebound_val = np.mean(os_traj[60:75]) if len(os_traj) > 75 else os_traj[-1]\n    if rebound_val > drop_val:\n        axs.annotate(\"rebound\", xy=(65, rebound_val),\n                     xytext=(75, rebound_val + 0.1),\n                     arrowprops=dict(arrowstyle=\"->\", color=\"C3\"),\n                     fontsize=10, color=\"C3\")\n\naxs.set_xlabel(\"Round\")\naxs.set_ylabel(r\"$\\theta$\")\naxs.set_title(\"One-shot constraint: drop and rebound\")\naxs.legend()\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "xnhqv1q7ysr",
   "source": "**Per-step analysis: do more steps help?**\n\nA counterintuitive result from the experimental data: more constraint injection steps do **not** reliably produce less drift. The coefficient of variation (CV) of per-step effects is high ($> 0.5$), meaning each step has a wildly different impact depending on the current system state. This is inconsistent with a reverse-diffusion denoising model where each step should contribute equally.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "use9p4p3em8",
   "source": "for n_steps_label in [\"4-step ramp\", \"8-step ramp\"]:\n    traj = trajectories[n_steps_label]\n    n_steps = 4 if \"4\" in n_steps_label else 8\n    step_rounds = [i * (n_rounds // (2 * n_steps)) for i in range(n_steps)]\n\n    deltas = []\n    for sr in step_rounds:\n        if sr < n_rounds - 5 and sr >= 3:\n            pre = np.mean(traj[max(0, sr - 3):sr])\n            post = np.mean(traj[sr:min(n_rounds, sr + 5)])\n            deltas.append(post - pre)\n\n    if deltas:\n        deltas = np.array(deltas)\n        nonzero = deltas[np.abs(deltas) > 1e-6]\n        if len(nonzero) > 1:\n            cv = np.std(nonzero) / np.abs(np.mean(nonzero))\n            print(f\"  {n_steps_label}: per-step deltas = \"\n                  f\"{[f'{d:.4f}' for d in deltas]}\")\n            print(f\"    CV = {cv:.2f}  \"\n                  f\"({'non-uniform (CV > 0.5)' if cv > 0.5 else 'approximately uniform'})\")\n        else:\n            print(f\"  {n_steps_label}: insufficient non-zero steps\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "l54hn57mh9n",
   "source": "**Summary**\n\nThis notebook demonstrated how to implement time-varying potentials in THRML by rebuilding the `IsingEBM` with updated biases at each sampling round and carrying forward the spin state.\n\n**Key findings from the schedule comparison:**\n\n1. **Persistent constraint massively outperforms** all other strategies. If you can apply the full constraint from the start, do so.\n2. **One-shot injection causes rebound** \u2014 the system partially recovers from sudden potential changes. This is a real experimental finding, not a simulation artifact.\n3. **More injection steps does not reliably mean less drift.** Per-step effects are state-dependent with high CV, inconsistent with equal-step denoising models.\n4. **Schedule shape matters more than step count.** The geometry of the constraint trajectory determines the outcome as much as the final constraint level.\n\n**The practical lesson:** When designing annealing or constraint schedules for THRML models, test multiple strategies. The assumption that gradual is always better than sudden (or vice versa) does not hold in general \u2014 the system's response to potential changes depends on its current state and history.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}